{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the task, I have decided to try the RL methods on the \"taxi-v3\" game. In this part, I have followed the Q-learning method to implement this. \n",
    "The rules of the game are as followed: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\users\\kushi arun kumar\\anaconda3\\envs\\pytorch\\lib\\site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\kushi arun kumar\\anaconda3\\envs\\pytorch\\lib\\site-packages (from gym) (1.19.5)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\users\\kushi arun kumar\\anaconda3\\envs\\pytorch\\lib\\site-packages (from gym) (6.7.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\kushi arun kumar\\anaconda3\\envs\\pytorch\\lib\\site-packages (from gym) (2.2.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\kushi arun kumar\\anaconda3\\envs\\pytorch\\lib\\site-packages (from gym) (0.0.8)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in c:\\users\\kushi arun kumar\\anaconda3\\envs\\pytorch\\lib\\site-packages (from importlib-metadata>=4.8.0->gym) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\kushi arun kumar\\anaconda3\\envs\\pytorch\\lib\\site-packages (from importlib-metadata>=4.8.0->gym) (3.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pygame in c:\\users\\kushi arun kumar\\anaconda3\\envs\\pytorch\\lib\\site-packages (2.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gym\n",
    "%pip install pygame\n",
    "#I started by installing the necessary packages, which is gym and pygame. Gym will be used to create and interact with the envirnoment for the taxi game.\n",
    "#pygame package is used in order to visualize the environemnt, while the first time running the proceeding code (to create the environment), it showed an error that it couldn't  reate the visualization of the game and hence it was suggested to import pygame as well to solve this bug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "import pygame\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "#here, I have imported all the libraries - numpy for arrays, random, gymnasium to create the environment, pygame, time for , IPython.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=gym.make(\"Taxi-v3\",render_mode='ansi')\n",
    "#this line of code is used to create the environment for the taxi game, we have used the function gym.make and we have rendered it in \"ansi\" mode which means that it will execute the visualization of the game for the human reader in text-based form \n",
    "#I also tried \"human\", for 'render_mode', but it was slow and not working at the best efficiency because it is very resource heavy hence I decided to opt for \"ansi\" mode instead as it is more lightweight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action size 6\n",
      "State size 500\n"
     ]
    }
   ],
   "source": [
    "#now the following code is used to craeate a Q-table - a Q-table is used to store the Q-values for each state-action pair\n",
    "action_size=env.action_space.n #retreiving the number of actions in the action set 'A', this will be used to create the number of rows.\n",
    "print(\"Action size\", action_size)\n",
    "#number possible actions possible - 6(up, down, left, right, drop person, pickup person)\n",
    "\n",
    "state_size=env.observation_space.n #retrieving the number of different elements in the States 'S' set, this will be used for the number of columnns\n",
    "print(\"State size\",state_size)\n",
    "#the different "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#here all the values in the Q-table are being initialized to zero, this allows the agent to explore the environment and update the q-values ccordingly so that it can exploit the evironment.\n",
    "qtable=np.zeros((state_size,action_size))\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we are defining the different variables\n",
    "total_episodes=1000 #this is the number of episodes used to train the model\n",
    "total_test_episodes=100 #this is the total number of episodes used to test the model \n",
    "max_steps=100 #this is the maximum number of steps per episodes\n",
    "\n",
    "learning_rate=0.3 #this is the learing rate - alpha - for updating the Q-values\n",
    "discount_rate=0.625 #this is the discount rate that is used for future rewards\n",
    "\n",
    "exploration_rate=1.0 #this is the initial exploration rate for teh epsilon-greedy strategy\n",
    "max_exploration_rate=1.0 #min and max exploration rates are the limits for the rate\n",
    "min_exploration_rate=0.01\n",
    "decay_rate=0.01 #decay rate for the exploration rate - how quickly it decays over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_all_episodes=[] #to store all the reawrds collected in each episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rewards per thousand episodes\n",
      "\n",
      "1000 : -55.33799999999997\n",
      "\n",
      "\n",
      "******Q-table******\n",
      "\n",
      "[[ 0.          0.          0.          0.          0.          0.        ]\n",
      " [-1.84607645 -1.76073166 -1.87374241 -1.71938203 17.24493554 -8.87239265]\n",
      " [-1.03855559 -0.99888289 -1.12395053 -0.96366375 18.74999902 -2.71540459]\n",
      " ...\n",
      " [-0.552375   19.12499964 -0.63844875 -0.59335399 -4.815      -2.07688129]\n",
      " [-1.0125     -0.95851149 -0.97875    -1.04533027 -6.3855     -2.8125    ]\n",
      " [-0.225      -0.225      -0.225      13.875345   -2.8125     -2.8125    ]]\n"
     ]
    }
   ],
   "source": [
    "for episode in range(total_episodes): #loop to run it 1000 times, which is also = to the total number of episodes for training\n",
    "    state=env.reset()[0] #this resets the environment entirely and sets the state variable to the the first element of the returned tuple\n",
    "    done=False #done represents whether the episode is finished or not yet\n",
    "    rewards_current_episode=0 #to keep trak of the rewards within the current episode, iti s set to 0 as we start off each episode with no rewards \n",
    "\n",
    "    for step in range(max_steps):\n",
    "\n",
    "        exploration_rate_threshold=random.uniform(0,1) #here we have used a function from the random library to generate a random number that will decide whether teh agent will explore or exploit\n",
    "        if exploration_rate_threshold>exploration_rate: #if the randomly generated number is greater than the exploration rate, then the agent will exploit the environment\n",
    "            action=np.argmax(qtable[state,:]) #the agent chooses the action with the highest Q-value for the next action (thus exploiting the information it has)\n",
    "        else:\n",
    "            action=env.action_space.sample() #this chooses a random action in space to perform as the agent is exploring\n",
    "\n",
    "        new_state, reward, done, truncated, info = env.step(action) #after choosing the action to perform, this line will actually execute the action and retrieve the info regarding the new state, the rewards, whether the episode is done or not, whether the episode has been truncated, and any additional info \n",
    "\n",
    "        qtable[state,action]=qtable[state,action]*(1-learning_rate)+learning_rate*(reward+discount_rate+np.max(qtable[new_state, :])) #this updates the Q value for the particular state-action pair in the q-table according to equation\n",
    "\n",
    "        state=new_state #updates the current step to the new state\n",
    "        rewards_current_episode+=reward \n",
    "\n",
    "        if done==True: #if the episode is done, the loop is broken\n",
    "            break\n",
    "\n",
    "    \n",
    "\n",
    "    exploration_rate=min_exploration_rate+(max_exploration_rate-min_exploration_rate)*np.exp(-decay_rate*episode) #this line of code here, updates the exploration rate according to th eparticular equation so that over time, the exploration rate is decreased thus gradually changing exploration to exploitation\n",
    "\n",
    "    rewards_all_episodes.append(rewards_current_episode) #so this line of code, appends the the rewards in each episode - the rewards in the current episode to a list named \"rewards_all_episodes\"\n",
    "\n",
    "\n",
    "rewards_per_thousand_episodes=np.split(np.array(rewards_all_episodes),total_episodes/1000) #This splits the rewards into chunks of 1000 episodes each. Since total_episodes is 1000, it results in one chunk.\n",
    "\n",
    "count=1000\n",
    "\n",
    "print(\"Average rewards per thousand episodes\\n\") #the block of code calculates and finds out the average rewards per thousand episodes, iterates ovver each chunk and computes the average rewards\n",
    "for r in rewards_per_thousand_episodes:\n",
    "    print(count,\":\",str(sum(r/1000)))\n",
    "    count+=1000\n",
    "\n",
    "print(\"\\n\\n******Q-table******\\n\")\n",
    "print(qtable) #printing the final q-table with the updated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35m\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "You reached the goal\n"
     ]
    }
   ],
   "source": [
    "#for this part of the code, we are trying to observe the performance of the agent in the environment, hence I created a loop to run the agent's performance 3 times\n",
    "for episode in range(3): \n",
    "    state = env.reset()[0] \n",
    "    done=False\n",
    "    print(\"Episode \",episode+1,\"\\n\\n\\n\\n\")\n",
    "    time.sleep(1) #pauses the execution for 1 second\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        clear_output(wait=True) #Clears the output in the current cell, particularly used in java\n",
    "        print(env.render()) #to render the final state of the environment in ansi form\n",
    "        time.sleep(0.3)\n",
    "\n",
    "        action=np.argmax(qtable[state,:])\n",
    "        new_state, reward, done, truncated, info=env.step(action)\n",
    "\n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            print(env.render())\n",
    "            if reward==20:\n",
    "                print(\"You reached the goal\")\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"You didnt\")\n",
    "                time.sleep(3)\n",
    "            clear_output(wait=True)\n",
    "            break\n",
    "\n",
    "        state=new_state\n",
    "\n",
    "env.close() #closes the environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
